section .data
    ; Maze stored as a flat 1D array (6x6 grid)
    maze db '.', '.', '.', '.', '.', 'S', \
              '.', 'X', 'X', '.', 'X', '.', \
              '.', '.', 'X', '.', 'X', '.', \
              'X', '.', 'X', '.', '.', '.', \
              'X', '.', 'X', 'X', 'X', '.', \
              '.', '.', '.', 'G', 'X', 'X'

    maze_width  equ 6
    maze_height equ 6

    ; Starting position
    start_x db 0
    start_y db 5

    ; Goal position
    goal_x  db 5
    goal_y  db 3

    ; Q-table: 144 floating-point entries (6x6 grid with 4 actions)
    q_table times 144 dd 0.0

    alpha   dd 0.1  ; Learning rate
    gamma   dd 0.9  ; Discount factor
    epsilon dd 0.1  ; Exploration rate

    ; Random number seed
    rand_seed dd 12345678

    newline db 10              ; Newline character

section .bss
    ; State variables to hold agent's current position
    state_x resb 1
    state_y resb 1

    ; Variable to track steps taken
    steps_taken resb 1

    ; Variable to store chosen action
    chosen_action resb 1

    ; Variable to store reward
    reward resb 1

section .text
    extern printf
    extern srand
    extern rand

    global _start

_start:
    ; Initialize random seed for exploration
    mov eax, [rand_seed]
    call srand

    ; Initialize agent's position to start (0, 5)
    mov al, [start_x]
    mov [state_x], al
    mov al, [start_y]
    mov [state_y], al
    mov byte [steps_taken], 0

    ; Training loop
train:
    ; Choose action using ε-greedy policy
    call choose_action

    ; Take the chosen action and get the reward
    call perform_action

    ; Update the Q-value using the Bellman equation
    call update_q_value

    ; Render the maze with the agent's position
    call render_maze

    ; Check if the agent has reached the goal
    cmp byte [state_x], [goal_x]
    jne not_goal
    cmp byte [state_y], [goal_y]
    jne not_goal

    ; Agent reached the goal
    mov byte [reward], 1  ; Set reward to 1 (goal reached)
    jmp end_program

not_goal:
    ; Agent did not reach the goal, keep training
    jmp train

end_program:
    ; Infinite loop to prevent program exit
    jmp end_program

; Choose action using ε-greedy policy
choose_action:
    ; Generate a random number to decide between exploration or exploitation
    call rand
    mov ecx, 100  ; Scaling random number between 0 and 100
    xor edx, edx
    div ecx
    cmp eax, dword [epsilon * 100] ; Scale epsilon for comparison
    jb explore  ; If random number < ε, explore (random action)

    ; Exploit: Choose the action with the highest Q-value
    call find_best_action
    jmp end_choose_action

explore:
    ; Randomly select one of the 4 actions
    call rand
    xor edx, edx
    mov ecx, 4  ; 4 possible actions (0: up, 1: down, 2: left, 3: right)
    div ecx
    mov [chosen_action], dl

end_choose_action:
    ret

; Find the action with the highest Q-value for the current state
find_best_action:
    ; Load current state (state_x, state_y)
    mov al, [state_x]
    mov bl, [state_y]

    ; Compute the Q-table index for the current state
    mov cl, maze_width
    mul cl         ; Multiply state_x by maze width
    add ax, bx     ; Add state_y to get the base index for the current state

    ; Check all 4 actions for the maximum Q-value
    mov ecx, 4
    xor esi, esi   ; Best action index
    mov ebx, q_table + eax * 4 ; Base address for Q-values

    ; Start with the first action
    fld dword [ebx] ; Load Q-value for the first action
    mov edi, 0      ; Initialize best action index to 0

find_best_loop:
    ; Load the next Q-value into the FPU
    fld dword [ebx + esi * 4]
    fcom
    fstsw ax
    sahf
    jae next_action  ; If current Q-value is larger or equal, skip updating

    mov edi, esi      ; Update best action index

next_action:
    inc esi
    cmp esi, 4
    jne find_best_loop

    ; Store the best action in chosen_action
    mov [chosen_action], edi
    ret

; Perform the chosen action, update the agent's state, and return the reward
perform_action:
    ; Load current position
    mov al, [state_x]
    mov bl, [state_y]

    ; Determine new position based on action
    mov cl, [chosen_action] ; Load chosen action

    ; Check action and update position
    cmp cl, 0          ; Up
    je move_up
    cmp cl, 1          ; Down
    je move_down
    cmp cl, 2          ; Left
    je move_left
    cmp cl, 3          ; Right
    je move_right

    jmp done_moving

move_up:
    dec bl              ; Move up (decrease row)
    jmp validate_move

move_down:
    inc bl              ; Move down (increase row)
    jmp validate_move

move_left:
    dec al              ; Move left (decrease column)
    jmp validate_move

move_right:
    inc al              ; Move right (increase column)

validate_move:
    ; Check for walls and boundaries
    cmp al, 0          ; Check left boundary
    jb invalid_move
    cmp al, maze_width ; Check right boundary
    jae invalid_move
    cmp bl, 0          ; Check upper boundary
    jb invalid_move
    cmp bl, maze_height; Check lower boundary
    jae invalid_move

    ; Check for walls
    mov dl, [maze + bl * maze_width + al]
    cmp dl, 'X'        ; Check if the new position is a wall
    je invalid_move

    ; Update agent's position if valid
    mov [state_x], al
    mov [state_y], bl
    mov byte [reward], -1 ; Regular move penalty
    jmp done_moving

invalid_move:
    mov byte [reward], -1 ; Regular move penalty

done_moving:
    ; Check if goal is reached
    cmp byte [state_x], [goal_x]
    jne not_goal_action
    cmp byte [state_y], [goal_y]
    jne not_goal_action

    ; Goal is reached
    mov byte [reward], 1
not_goal_action:
    ret

; Update Q-value using the Bellman equation
update_q_value:
    ; Load current state (state_x, state_y)
    mov al, [state_x]
    mov bl, [state_y]

    ; Compute the index in the Q-table
    mov cl, maze_width
    mul cl
    add ax, bx
    mov esi, eax
    add esi, [chosen_action] ; Add chosen action to the base index

    ; Load current Q-value
    fld dword [q_table + esi * 4]

    ; Load the reward
    fld dword [reward]

    ; Prepare to find max Q-value for the next state
    ; Simulate the next state based on chosen action
    mov cl, [chosen_action] ; Load chosen action

    ; Adjust the state based on action to find new state
    mov al, [state_x]
    mov bl, [state_y]
    cmp cl, 0
    je move_up_update
    cmp cl, 1
    je move_down_update
    cmp cl, 2
    je move_left_update
    cmp cl, 3
    je move_right_update

move_up_update:
    dec bl              ; Move up
    jmp get_max_q_value

move_down_update:
    inc bl              ; Move down
    jmp get_max_q_value

move_left_update:
    dec al              ; Move left
    jmp get_max_q_value

move_right_update:
    inc al              ; Move right
    jmp get_max_q_value

get_max_q_value:
    ; Validate the new position
    cmp al, 0          ; Check left boundary
    jb set_max_q_zero
    cmp al, maze_width ; Check right boundary
    jae set_max_q_zero
    cmp bl, 0          ; Check upper boundary
    jb set_max_q_zero
    cmp bl, maze_height; Check lower boundary
    jae set_max_q_zero

    ; Check for walls
    mov dl, [maze + bl * maze_width + al]
    cmp dl, 'X'        ; Check if the new position is a wall
    je set_max_q_zero

    ; New position is valid; calculate index in Q-table
    mov cx, bl
    mov dx, al
    mov bx, maze_width
    mul bx              ; Calculate new state index
    add ax, dx          ; Add new state column index
    shl eax, 2          ; Multiply by 4 (size of float)

    ; Load max Q-value from new state for all actions
    mov ecx, 4
    xor edi, edi
    fld dword [q_table + eax] ; Load first Q-value
    fld dword [q_table + eax + 4] ; Load second Q-value
    fcomip st(0), st(1)
    fstp st(0)
    jae load_first_max

    load_first_max:
    fld dword [q_table + eax + 8] ; Load third Q-value
    fcomip st(0), st(1)
    fstp st(0)
    jae load_second_max

    load_second_max:
    fld dword [q_table + eax + 12] ; Load fourth Q-value
    fcomip st(0), st(1)
    fstp st(0)
    jae done_max_q

    done_max_q:
    ; Now we have the max Q-value in ST(0)
    ; Bellman update: Q(s, a) = Q(s, a) + α * (reward + γ * max(Q(s', a')) - Q(s, a))

    fld dword [reward]
    fld dword [gamma]
    fmul st(0), st(1)         ; α * (reward + γ * max)
    fld dword [alpha]         ; Load α
    fmul st(0), st(1)         ; α * (reward + γ * max)
    fsub st(0), st(2)         ; Calculate Q(s, a) - max
    fadd st(1), st(0)         ; Add max Q-value

    ; Store updated Q-value back in the Q-table
    fstp dword [q_table + esi * 4]
    ret

; Render the maze with the agent's position
render_maze:
    ; Print the maze to the console with the agent marked
    mov ecx, maze_height
    xor ebx, ebx ; Row index

print_loop:
    push ecx ; Save loop counter
    mov edx, maze_width
    xor edi, edi ; Column index

print_row:
    cmp edi, maze_width
    jge print_newline

    ; Calculate index in maze array
    mov eax, ebx
    mov ecx, maze_width
    mul ecx
    add eax, edi

    ; Check if this is the agent's position
    mov al, [state_x]
    cmp al, edi
    jne not_agent
    mov al, [state_y]
    cmp al, ebx
    je print_agent

not_agent:
    ; Print the maze character
    mov al, [maze + eax]
    mov [buffer], al
    mov eax, buffer
    call printf
    jmp print_increment

print_agent:
    ; Print agent symbol
    mov al, 'A'
    mov [buffer], al
    mov eax, buffer
    call printf

print_increment:
    inc edi
    jmp print_row

print_newline:
    ; Print a newline after each row
    mov eax, newline
    call printf

    pop ecx ; Restore loop counter
    inc ebx
    cmp ebx, maze_height
    jl print_loop

    ret

section .data
    buffer db 0   ; Buffer for single character print
